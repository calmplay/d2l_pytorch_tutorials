{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# derivative & gradient",
   "id": "114adc2b3aa2c00"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 自动求导\n",
    "假设要对$y=2x^Tx$关于列向量$x$求导"
   ],
   "id": "6c0bb01d971675af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T06:19:18.714892Z",
     "start_time": "2024-10-14T06:19:17.206415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(4.0)\n",
    "x"
   ],
   "id": "c7e8d2198463104f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T06:19:18.723323Z",
     "start_time": "2024-10-14T06:19:18.717510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x.requires_grad_(True)  # 同x=torch.arange(4.0,requires_grad=True) \n",
    "x.requires_grad  # Instance attribute requires_grad"
   ],
   "id": "e56422c58949991f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T06:19:18.730121Z",
     "start_time": "2024-10-14T06:19:18.724766Z"
    }
   },
   "cell_type": "code",
   "source": "x.grad  # 梯度gradient,默认是None",
   "id": "268ec84acbcb8329",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T06:19:18.740120Z",
     "start_time": "2024-10-14T06:19:18.731439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y = 2 * torch.dot(x, x)\n",
    "y"
   ],
   "id": "fdee581952214877",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T06:19:18.869011Z",
     "start_time": "2024-10-14T06:19:18.743142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 调用反向传播来自动计算y关于x每个分量的梯度\n",
    "y.backward()\n",
    "x.grad"
   ],
   "id": "11b86830aae9d7cb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T06:19:18.875893Z",
     "start_time": "2024-10-14T06:19:18.870411Z"
    }
   },
   "cell_type": "code",
   "source": "x.grad == 4 * x",
   "id": "f122e3b70f3bbca1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T06:19:18.883785Z",
     "start_time": "2024-10-14T06:19:18.877077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 默认情况下,PyTorch会累积梯度,我们需要清除之前的值\n",
    "x.grad.zero_()\n",
    "y = x.sum()  # 一个新的函数\n",
    "y.backward()\n",
    "x.grad\n",
    "# 如果不清0, 把x.grad.zero_()注释,结果为:tensor([ 1.,  5.,  9., 13.])"
   ],
   "id": "dd5b62b6c2848aa5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T06:19:19.378085Z",
     "start_time": "2024-10-14T06:19:18.885054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# (机器学习中,很少对非标量进行一个backward)\n",
    "# 对于非标量调用backward需要传入一个gradient参数,该参数指定微分函数\n",
    "x.grad.zero_()\n",
    "y = x * x  # 注意: 这里的*运算符被torch重写了\n",
    "# 现在y是一个矩阵,直接对其backward会报错\n",
    "# y.backward()  # Error:RuntimeError: grad can be implicitly(隐式)\n",
    "#                 created only for scalar(标量) outputs\n",
    "\n",
    "#print(\"了解retain_graph的作用,监视y.sum():\\n\",y.sum(),y.sum())\n",
    "y.sum().backward(retain_graph=True)  # <=>y.backward(torch.ones(len(x)))\n",
    "# retain_graph=True表示不清除计算图,这样中间的算子(eg:y)即可再次backward\n",
    "print(x.grad)\n",
    "print(y)\n",
    "x.grad.zero_()  # <=> y.backward(torch.ones(len(x)))\n",
    "y.backward(gradient=torch.ones(len(x)))  # <=> y.sum().backward()\n",
    "# 参数gradient表示给self一个梯度,这里给一个[1,1,1,1],正向走一步<=>sum(self)\n",
    "print(x.grad)\n",
    "print(y)\n",
    "#print(\"了解retain_graph的作用,监视y.sum():\\n\",y.sum())"
   ],
   "id": "ab92887e6c78f835",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 2., 4., 6.])\n",
      "tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>)\n",
      "tensor([0., 2., 4., 6.])\n",
      "tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```\n",
    "Tensor.backward(.) Args:\n",
    "            gradient (Tensor, optional): The gradient of the function\n",
    "                being differentiated w.r.t. ``self``.\n",
    "                This argument can be omitted if ``self`` is a scalar.\n",
    "            retain_graph (bool, optional): If ``False``, the graph used to compute\n",
    "                the grads will be freed. Note that in nearly all cases setting\n",
    "                this option to True is not needed and often can be worked around\n",
    "                in a much more efficient way. Defaults to the value of\n",
    "                ``create_graph``.\n",
    "            create_graph (bool, optional): If ``True``, graph of the derivative will\n",
    "                be constructed, allowing to compute higher order derivative\n",
    "                products. Defaults to ``False``.\n",
    "            inputs (sequence of Tensor, optional): Inputs w.r.t. which the gradient will be\n",
    "                accumulated into ``.grad``. All other tensors will be ignored. If not\n",
    "                provided, the gradient is accumulated into all the leaf Tensors that were\n",
    "                used to compute the :attr:`tensors`.\n",
    "```"
   ],
   "id": "7886ce6470a80aaa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T06:19:19.388170Z",
     "start_time": "2024-10-14T06:19:19.380446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 将某些计算移动到记录的计算图之外\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach()  # 创建新张量,不再追踪梯度信息(分离后的张量操作不会影响y)\n",
    "# 即 u=[0,1,4,9],与x无关\n",
    "u.requires_grad_()\n",
    "z = u * x\n",
    "z.sum().backward()  # 生成一个反向计算图,记z.sum()为r,即计算图(树)的根\n",
    "x.grad == u\n",
    "# 分析: 记r=z.sum()=sum(u⊙x)\n",
    "# x是向量(默认为列向量),u是独立于x的另一个向量\n",
    "# 对x求偏导,u与x无关,u可看成是一个常数向量\n",
    "# ∴x.grad=∂r/∂x = ∂r/∂z * ∂z/∂x = 1^T*∂(u⊙x)/∂x = u\n",
    "# 注意,现在x的梯度是∂r/∂x, 不是之前的∂(x*x)/∂x"
   ],
   "id": "afe7648725e321af",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T06:19:19.396982Z",
     "start_time": "2024-10-14T06:19:19.390099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "x.grad == 2 * x"
   ],
   "id": "46e18e6caac90db7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T06:19:19.407570Z",
     "start_time": "2024-10-14T06:19:19.398463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 即使是走python控制流, backward依然可行\n",
    "def f(num):\n",
    "    b = num * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c\n",
    "\n",
    "\n",
    "a = torch.randn(size=(), requires_grad=True)  # size=() means that the tensor will be a scalar\n",
    "d = f(a)\n",
    "d.backward()\n",
    "a.grad"
   ],
   "id": "e4505156a560bbe2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(204800.)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### 其他"
   ],
   "id": "7a5173a27092868a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T06:19:19.721156Z",
     "start_time": "2024-10-14T06:19:19.409067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a1 = torch.tensor([1.], requires_grad=True)\n",
    "a2 = a1 * a1\n",
    "a3 = a1.sum()\n",
    "a3.backward()\n",
    "print(a1.grad)\n",
    "a1.grad.zero_()\n",
    "a3.backward()\n",
    "print(a1.grad)\n",
    "\n",
    "v1 = torch.tensor([1, 2.], requires_grad=True)\n",
    "v2 = v1 * v1\n",
    "v3 = v2.sum()\n",
    "# v3.backward(retain_graph=True)\n",
    "v3.backward()\n",
    "print(v1.grad)\n",
    "v1.grad.zero_()\n",
    "v3.backward()  # Error (要进行多次backward,上一次时必须加retain_graph=True)\n",
    "print(v1.grad)\n",
    "\n",
    "# 但是标量的就没有这个问题,所以retain_graph保留计算图的作用是什么?\n"
   ],
   "id": "59bd0136b1e141c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([2., 4.])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 17\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28mprint\u001B[39m(v1\u001B[38;5;241m.\u001B[39mgrad)\n\u001B[1;32m     16\u001B[0m v1\u001B[38;5;241m.\u001B[39mgrad\u001B[38;5;241m.\u001B[39mzero_()\n\u001B[0;32m---> 17\u001B[0m \u001B[43mv3\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Error (要进行多次backward,上一次时必须加retain_graph=True)\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(v1\u001B[38;5;241m.\u001B[39mgrad)\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# 但是标量的就没有这个问题,所以retain_graph保留计算图的作用是什么?\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/cy_env_py38pt24/lib/python3.8/site-packages/torch/_tensor.py:521\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    512\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    513\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    514\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    519\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    520\u001B[0m     )\n\u001B[0;32m--> 521\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    522\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/cy_env_py38pt24/lib/python3.8/site-packages/torch/autograd/__init__.py:289\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    284\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    286\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    288\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 289\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    290\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    291\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    292\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    293\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    294\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    295\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    296\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    297\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/cy_env_py38pt24/lib/python3.8/site-packages/torch/autograd/graph.py:769\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    767\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    768\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 769\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    770\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    771\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    772\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    773\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "35d92902079847f9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
